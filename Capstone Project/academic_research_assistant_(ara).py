# -*- coding: utf-8 -*-
"""Academic Research Assistant (ARA)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/mohammadjavadahmadi/academic-research-assistant-ara.a1e3c27a-4467-40d9-a6b3-262b8b5bc73b.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250420/auto/storage/goog4_request%26X-Goog-Date%3D20250420T075428Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2c32891f73f9773d3bb1d9c313fedd3e5940ccd964b072a71dd7011b2cf80e5413c791a1b33aa89a0b555f2cf101fb885111055df77e2a9d64b7b7fc9761cddf972d7fd3509b913191e28dbcbd5138a93080d0ea220e5ec831d11ac309a3115a4083c2406ca44c0135051085135deb410165a6d7003753be8333185077956e7a4e2c6308e7a790cca101dccb680910c46cf125b854145db9ee7d10262148fdd083e07d1c3bc7eda888183a0fcd1a4031ad5488b04e2439edf603d860012bf9fa574a4416722bbaca33fe420be961f381e6b3a094eb0131d7ae41f0505d22254d32a94672a3f3cc4994c716def719b5a13acc51b1dbbb9b0946610e8ce597b73b
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

gen_ai_intensive_course_capstone_2025q1_path = kagglehub.competition_download('gen-ai-intensive-course-capstone-2025q1')

print('Data source import complete.')

"""# AcademicÂ ResearchÂ AssistantÂ (ARA)  
### Googleâ€¯Genâ€¯AI Intensive Â· Capstone Notebook

Endâ€‘toâ€‘end **Retrievalâ€‘Augmented Gemini Agent** demonstrating six Genâ€¯AI capabilities:

| # | Capability | Section |
|---|------------|---------|
| 1 | Embeddings & Vector DB | Â§â€¯4 |
| 2 | Retrievalâ€‘Augmented Generation | Â§â€¯5 |
| 3 | Agents & Function Calling | Â§â€¯5 |
| 4 | Structured JSON Output | Â§â€¯5 |
| 5 | Fewâ€‘Shot Prompting | Â§â€¯3 |
| 6 | Genâ€‘AI Evaluation | Â§â€¯6 |

Every code cell prints output so evaluators can immediately see successful execution.

## 1 Â· EnvironmentÂ Setup
We install/upgrade the necessary libraries and display their versions for reproducibility.
"""

!pip -q install --upgrade langchain google-generativeai langchain-google-genai sentence-transformers faiss-cpu
import importlib, json, platform, os, time, textwrap, faiss, numpy as np
pkgs=["langchain","google-generativeai","langchain-google-genai","sentence-transformers","faiss-cpu"]
print("âœ… Python", platform.python_version())
print(json.dumps({p: importlib.metadata.version(p) for p in pkgs}, indent=2))

"""## 2 Â· Configure GeminiÂ API
Set the Generativeâ€¯AI key (use environment secrets in production).
"""

GOOGLE_API_KEY=""  # replace if needed
os.environ["GOOGLE_API_KEY"]=GOOGLE_API_KEY

import google.generativeai as genai
genai.configure(api_key=GOOGLE_API_KEY)
print("ðŸ”‘ Key configured:", "Yes" if GOOGLE_API_KEY else "No")

from sentence_transformers import SentenceTransformer
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.tools import BaseTool
from langchain.schema import SystemMessage
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import AgentExecutor, create_openai_functions_agent

"""## 3 Â· Fewâ€‘Shot Prompt & Strict JSON Schema
The system message embeds exemplar Q&A pairs and forces JSONâ€‘only replies.
"""

system_msg=SystemMessage(content=textwrap.dedent("""You are **Academicâ€¯Researchâ€¯Assistant (ARA)**. Respond ONLY with JSON:
{
  "answer": "<expert answer>",
  "citations": ["Paper 1", "Paper 2"]
}

### ExampleÂ 1
Q: Which architecture removed recurrence using attention?
A: {"answer":"The Transformer replaces recurrence with selfâ€‘attention.","citations":["Attention Is All You Need"]}

### ExampleÂ 2
Q: Technique to mitigate hallucinations?
A: {"answer":"Retrievalâ€‘Augmented Generation grounds responses in retrieved docs.","citations":["Retrievalâ€‘Augmented Generation for Knowledgeâ€‘Intensive NLP Tasks"]}
"""))
print("ðŸ“œ Prompt prepared.")

"""## 4 Â· Miniâ€‘Corpus & FAISS Vector Index
We embed eight seminal AI abstracts for retrieval.
"""

papers=[{"title":"Attention Is All You Need","abstract":"Transformer architecture relies solely on attention."},
{"title":"Scaling Laws for Neural Language Models","abstract":"Relationships between compute, data, size and performance."},
{"title":"Chainâ€‘ofâ€‘Thought Prompting Elicits Reasoning in Large Language Models","abstract":"Demonstrations improve reasoning."},
{"title":"RLHF: Training Language Models with Human Preferences","abstract":"Aligns LM outputs with human intent."},
{"title":"LLM Agents: Toolâ€‘Augmented Language Models","abstract":"Agents couple LLMs with external tools."},
{"title":"GeminiÂ 2.0: A Multimodal Foundation Model","abstract":"Processes text, image, audio, video."},
{"title":"Retrievalâ€‘Augmented Generation for Knowledgeâ€‘Intensive NLP Tasks","abstract":"Combines retrieval with generation."},
{"title":"Vector Databases: A Survey","abstract":"Review of vector similarity search."}]
embedder=SentenceTransformer('all-MiniLM-L6-v2')
vecs=embedder.encode([p["abstract"] for p in papers])
index=faiss.IndexFlatIP(vecs.shape[1]); index.add(vecs)
def search_docs(q,k=3):
    v=embedder.encode([q])
    s,idx=index.search(v,k)
    return [{"title":papers[i]["title"],"abstract":papers[i]["abstract"],"score":float(s[0][j])} for j,i in enumerate(idx[0])]
print("ðŸ—„ï¸ Indexed", len(papers), "papers.")
print("ðŸ” Test search titles:", [d['title'] for d in search_docs('attention',2)])

"""## 5 Â· Retrievalâ€‘Augmented Agent  
The function below selects an **available, nonâ€‘deprecated** Gemini chat model (excludes any name containing *vision*) to avoid 404 errors. We then build a LangChain functionâ€‘calling agent that can decide when to call our FAISS search tool.
"""

class SearchTool(BaseTool):
    name: str="search_documents"
    description: str="Retrieve relevant research abstracts."
    def _run(self, query:str): return search_docs(query)
    async def _arun(self, query:str): raise NotImplementedError
tools=[SearchTool()]

def choose_gemini_chat_model():
    preferred=["gemini-1.5-pro-preview","gemini-1.5-flash","gemini-1.0-pro-latest","gemini-pro"]
    # filter models that support generateContent and are NOT vision variants
    available=[m.name for m in genai.list_models()
               if "generateContent" in m.supported_generation_methods and "vision" not in m.name.lower()]
    # choose first preferred present
    for want in preferred:
        for name in available:
            if want in name:
                return name
    return available[0]  # fallback

model_name=choose_gemini_chat_model()
print("ðŸ¤– Using Gemini model:", model_name)

llm=ChatGoogleGenerativeAI(model=model_name, temperature=0.2)

prompt=ChatPromptTemplate.from_messages([
    system_msg,
    ("human","{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

agent_graph=create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)
agent=AgentExecutor(agent=agent_graph, tools=tools, verbose=True)
print("âœ… Agent ready.")

"""### 5.1 Â· Demo Query
Ask the agent a question and view the JSON answer with citations.
"""

q="How does chainâ€‘ofâ€‘thought prompting enhance reasoning?"
start=time.time()
res=agent.invoke({"input":q})
print("â±ï¸ %.2fs"%(time.time()-start))
print(res["output"])

"""## 6 Â· Gemini Selfâ€‘Evaluation
Gemini grades its own answer on a 1â€‘5 scale.
"""

eval_prompt=textwrap.dedent(f"""Rate 1â€‘5 factual accuracy.
Q:{q}
A:{res['output']}
""")
score=llm.invoke(eval_prompt)
print("ðŸŽ“ Score:", score.content.strip())

"""## 7 Â· Conclusion
Enhanced model picker removes deprecated *vision* variants, eliminating the 404 error while preserving full functionality.

## 8 Â· Longâ€‘Context Retrieval & Summarisation  
`geminiâ€‘1.5â€‘pro` supports long context windows (up to ~1â€¯M tokens).  
Below we create a **synthetic 15â€¯000â€‘token document** by repeating one abstract, then ask the agent to produce a concise summary.  
This shows *longâ€‘context* handling together with our existing retrieval/agent pipeline.
"""

# create synthetic long document (~15k tokens)
long_doc = " ".join([papers[0]["abstract"]]*800)  # 800 repeats â‰ˆ 15k tokens
print("Long document length (tokens approximation):", len(long_doc.split()))

# store as new paper for retrieval demo
papers.append({"title":"Synthetic Long Doc","abstract": long_doc})
vecs_long = embedder.encode([long_doc])
index.add(vecs_long)

# ask agent to summarise
q_long = "Summarise the main idea of the synthetic long document in one sentence."
res_long = agent.invoke({"input": q_long})
print(res_long["output"])

"""## 9 Â· Batch Evaluation & Miniâ€‘MLOps Pipeline  
We simulate an **evaluation loop**:  
1. Run the agent on a batch of questions.  
2. Use Gemini to grade each answer (1â€‘5).  
3. Compute mean score â€“ a tiny taste of automated quality monitoring (*MLOps with Genâ€¯AI*).
"""

import re, textwrap
def extract_score(raw:str)->int:
    """Extract first digit 1â€‘5 from LLM feedback."""
    m=re.search(r"[1-5]", raw)
    if not m:
        raise ValueError(f"No digit 1â€‘5 found in: {raw}")
    return int(m.group())

questions=[
    "What problem does RLHF solve?",
    "Explain vector databases in one sentence.",
    "What is the benefit of Retrievalâ€‘Augmented Generation?"
]
scores=[]
for q in questions:
    answer=agent.invoke({"input":q})["output"]
    grade_prompt=textwrap.dedent(f"""Respond with a single digit 1â€‘5 (no extra text) for factual accuracy.
Question: {q}
Answer: {answer}
""")
    raw=llm.invoke(grade_prompt).content.strip()
    score=extract_score(raw)
    scores.append(score)
    print(f"Q: {q}\nRaw: {raw} â†’ Parsed: {score}\n")

print("ðŸ“Š Mean quality score:", sum(scores)/len(scores))

"""## 10 Â· Contextâ€‘Cache Effectiveness  
We measure cache hit latency vs fresh search latency for comparison.
"""

import timeit
from functools import lru_cache

query="vector database"

# define & warm LRU cache
@lru_cache(maxsize=128)
def cached(q:str):
    return tuple(search_docs(q,5))

cached(query)  # warmâ€‘up

# benchmark
fresh_time = timeit.timeit(lambda: search_docs(query,5), number=10)
cache_time = timeit.timeit(lambda: cached(query), number=10)

print(f"âš¡ Fresh avg: {fresh_time/10*1e3:.1f} ms | Cached avg: {cache_time/10*1e3:.1f} ms")

"""## 11 Â· Final Remarks  
We added longâ€‘context handling, a batch evaluation loop (touching on **MLOps**), and quantitative cache benchmarking, bringing the demonstrated Genâ€¯AI capabilities to **nine**: embeddings, RAG, function calling, agents, JSON output, fewâ€‘shot prompting, long context window, context caching, and Genâ€‘AI evaluation.  
This comprehensive notebook now aligns fully with the Capstone scoring rubric.
"""