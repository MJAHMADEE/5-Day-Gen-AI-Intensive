{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 97258,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Academic Research Assistant (ARA)",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "aBa843muMR6A"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "gen_ai_intensive_course_capstone_2025q1_path = kagglehub.competition_download('gen-ai-intensive-course-capstone-2025q1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "AAyWBSceMR6C"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Academic¬†Research¬†Assistant¬†(ARA)  \n",
        "### Google‚ÄØGen‚ÄØAI Intensive ¬∑ Capstone Notebook\n",
        "\n",
        "End‚Äëto‚Äëend **Retrieval‚ÄëAugmented Gemini Agent** demonstrating six Gen‚ÄØAI capabilities:\n",
        "\n",
        "| # | Capability | Section |\n",
        "|---|------------|---------|\n",
        "| 1 | Embeddings & Vector DB | ¬ß‚ÄØ4 |\n",
        "| 2 | Retrieval‚ÄëAugmented Generation | ¬ß‚ÄØ5 |\n",
        "| 3 | Agents & Function Calling | ¬ß‚ÄØ5 |\n",
        "| 4 | Structured JSON Output | ¬ß‚ÄØ5 |\n",
        "| 5 | Few‚ÄëShot Prompting | ¬ß‚ÄØ3 |\n",
        "| 6 | Gen‚ÄëAI Evaluation | ¬ß‚ÄØ6 |\n",
        "\n",
        "Every code cell prints output so evaluators can immediately see successful execution."
      ],
      "metadata": {
        "id": "9fbe0465"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 ¬∑ Environment¬†Setup\n",
        "We install/upgrade the necessary libraries and display their versions for reproducibility."
      ],
      "metadata": {
        "id": "aeeff68a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade langchain google-generativeai langchain-google-genai sentence-transformers faiss-cpu\n",
        "import importlib, json, platform, os, time, textwrap, faiss, numpy as np\n",
        "pkgs=[\"langchain\",\"google-generativeai\",\"langchain-google-genai\",\"sentence-transformers\",\"faiss-cpu\"]\n",
        "print(\"‚úÖ Python\", platform.python_version())\n",
        "print(json.dumps({p: importlib.metadata.version(p) for p in pkgs}, indent=2))\n"
      ],
      "metadata": {
        "id": "a489c989"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 ¬∑ Configure Gemini¬†API\n",
        "Set the Generative‚ÄØAI key (use environment secrets in production)."
      ],
      "metadata": {
        "id": "9c8568d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY=\"\"  # replace if needed\n",
        "os.environ[\"GOOGLE_API_KEY\"]=GOOGLE_API_KEY\n",
        "\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "print(\"üîë Key configured:\", \"Yes\" if GOOGLE_API_KEY else \"No\")\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n"
      ],
      "metadata": {
        "id": "6b9fb72a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 ¬∑ Few‚ÄëShot Prompt & Strict JSON Schema\n",
        "The system message embeds exemplar Q&A pairs and forces JSON‚Äëonly replies."
      ],
      "metadata": {
        "id": "d315670d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_msg=SystemMessage(content=textwrap.dedent(\"\"\"You are **Academic‚ÄØResearch‚ÄØAssistant (ARA)**. Respond ONLY with JSON:\n",
        "{\n",
        "  \"answer\": \"<expert answer>\",\n",
        "  \"citations\": [\"Paper 1\", \"Paper 2\"]\n",
        "}\n",
        "\n",
        "### Example¬†1\n",
        "Q: Which architecture removed recurrence using attention?\n",
        "A: {\"answer\":\"The Transformer replaces recurrence with self‚Äëattention.\",\"citations\":[\"Attention Is All You Need\"]}\n",
        "\n",
        "### Example¬†2\n",
        "Q: Technique to mitigate hallucinations?\n",
        "A: {\"answer\":\"Retrieval‚ÄëAugmented Generation grounds responses in retrieved docs.\",\"citations\":[\"Retrieval‚ÄëAugmented Generation for Knowledge‚ÄëIntensive NLP Tasks\"]}\n",
        "\"\"\"))\n",
        "print(\"üìú Prompt prepared.\")\n"
      ],
      "metadata": {
        "id": "083eac35"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 ¬∑ Mini‚ÄëCorpus & FAISS Vector Index\n",
        "We embed eight seminal AI abstracts for retrieval."
      ],
      "metadata": {
        "id": "8d17df16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "papers=[{\"title\":\"Attention Is All You Need\",\"abstract\":\"Transformer architecture relies solely on attention.\"},\n",
        "{\"title\":\"Scaling Laws for Neural Language Models\",\"abstract\":\"Relationships between compute, data, size and performance.\"},\n",
        "{\"title\":\"Chain‚Äëof‚ÄëThought Prompting Elicits Reasoning in Large Language Models\",\"abstract\":\"Demonstrations improve reasoning.\"},\n",
        "{\"title\":\"RLHF: Training Language Models with Human Preferences\",\"abstract\":\"Aligns LM outputs with human intent.\"},\n",
        "{\"title\":\"LLM Agents: Tool‚ÄëAugmented Language Models\",\"abstract\":\"Agents couple LLMs with external tools.\"},\n",
        "{\"title\":\"Gemini¬†2.0: A Multimodal Foundation Model\",\"abstract\":\"Processes text, image, audio, video.\"},\n",
        "{\"title\":\"Retrieval‚ÄëAugmented Generation for Knowledge‚ÄëIntensive NLP Tasks\",\"abstract\":\"Combines retrieval with generation.\"},\n",
        "{\"title\":\"Vector Databases: A Survey\",\"abstract\":\"Review of vector similarity search.\"}]\n",
        "embedder=SentenceTransformer('all-MiniLM-L6-v2')\n",
        "vecs=embedder.encode([p[\"abstract\"] for p in papers])\n",
        "index=faiss.IndexFlatIP(vecs.shape[1]); index.add(vecs)\n",
        "def search_docs(q,k=3):\n",
        "    v=embedder.encode([q])\n",
        "    s,idx=index.search(v,k)\n",
        "    return [{\"title\":papers[i][\"title\"],\"abstract\":papers[i][\"abstract\"],\"score\":float(s[0][j])} for j,i in enumerate(idx[0])]\n",
        "print(\"üóÑÔ∏è Indexed\", len(papers), \"papers.\")\n",
        "print(\"üîç Test search titles:\", [d['title'] for d in search_docs('attention',2)])\n"
      ],
      "metadata": {
        "id": "3be729bb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 ¬∑ Retrieval‚ÄëAugmented Agent  \n",
        "The function below selects an **available, non‚Äëdeprecated** Gemini chat model (excludes any name containing *vision*) to avoid 404 errors. We then build a LangChain function‚Äëcalling agent that can decide when to call our FAISS search tool."
      ],
      "metadata": {
        "id": "18d97b9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchTool(BaseTool):\n",
        "    name: str=\"search_documents\"\n",
        "    description: str=\"Retrieve relevant research abstracts.\"\n",
        "    def _run(self, query:str): return search_docs(query)\n",
        "    async def _arun(self, query:str): raise NotImplementedError\n",
        "tools=[SearchTool()]\n",
        "\n",
        "def choose_gemini_chat_model():\n",
        "    preferred=[\"gemini-1.5-pro-preview\",\"gemini-1.5-flash\",\"gemini-1.0-pro-latest\",\"gemini-pro\"]\n",
        "    # filter models that support generateContent and are NOT vision variants\n",
        "    available=[m.name for m in genai.list_models()\n",
        "               if \"generateContent\" in m.supported_generation_methods and \"vision\" not in m.name.lower()]\n",
        "    # choose first preferred present\n",
        "    for want in preferred:\n",
        "        for name in available:\n",
        "            if want in name:\n",
        "                return name\n",
        "    return available[0]  # fallback\n",
        "\n",
        "model_name=choose_gemini_chat_model()\n",
        "print(\"ü§ñ Using Gemini model:\", model_name)\n",
        "\n",
        "llm=ChatGoogleGenerativeAI(model=model_name, temperature=0.2)\n",
        "\n",
        "prompt=ChatPromptTemplate.from_messages([\n",
        "    system_msg,\n",
        "    (\"human\",\"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "])\n",
        "\n",
        "agent_graph=create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)\n",
        "agent=AgentExecutor(agent=agent_graph, tools=tools, verbose=True)\n",
        "print(\"‚úÖ Agent ready.\")\n"
      ],
      "metadata": {
        "id": "62069753"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 ¬∑ Demo Query\n",
        "Ask the agent a question and view the JSON answer with citations."
      ],
      "metadata": {
        "id": "c99a4c11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q=\"How does chain‚Äëof‚Äëthought prompting enhance reasoning?\"\n",
        "start=time.time()\n",
        "res=agent.invoke({\"input\":q})\n",
        "print(\"‚è±Ô∏è %.2fs\"%(time.time()-start))\n",
        "print(res[\"output\"])\n"
      ],
      "metadata": {
        "id": "696f7d73"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 ¬∑ Gemini Self‚ÄëEvaluation\n",
        "Gemini grades its own answer on a 1‚Äë5 scale."
      ],
      "metadata": {
        "id": "9725dd1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt=textwrap.dedent(f\"\"\"Rate 1‚Äë5 factual accuracy.\n",
        "Q:{q}\n",
        "A:{res['output']}\n",
        "\"\"\")\n",
        "score=llm.invoke(eval_prompt)\n",
        "print(\"üéì Score:\", score.content.strip())\n"
      ],
      "metadata": {
        "id": "eac3a150"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 ¬∑ Conclusion\n",
        "Enhanced model picker removes deprecated *vision* variants, eliminating the 404 error while preserving full functionality."
      ],
      "metadata": {
        "id": "f4c9610f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8 ¬∑ Long‚ÄëContext Retrieval & Summarisation  \n",
        "`gemini‚Äë1.5‚Äëpro` supports long context windows (up to ~1‚ÄØM tokens).  \n",
        "Below we create a **synthetic 15‚ÄØ000‚Äëtoken document** by repeating one abstract, then ask the agent to produce a concise summary.  \n",
        "This shows *long‚Äëcontext* handling together with our existing retrieval/agent pipeline."
      ],
      "metadata": {
        "id": "cb15edcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create synthetic long document (~15k tokens)\n",
        "long_doc = \" \".join([papers[0][\"abstract\"]]*800)  # 800 repeats ‚âà 15k tokens\n",
        "print(\"Long document length (tokens approximation):\", len(long_doc.split()))\n",
        "\n",
        "# store as new paper for retrieval demo\n",
        "papers.append({\"title\":\"Synthetic Long Doc\",\"abstract\": long_doc})\n",
        "vecs_long = embedder.encode([long_doc])\n",
        "index.add(vecs_long)\n",
        "\n",
        "# ask agent to summarise\n",
        "q_long = \"Summarise the main idea of the synthetic long document in one sentence.\"\n",
        "res_long = agent.invoke({\"input\": q_long})\n",
        "print(res_long[\"output\"])\n"
      ],
      "metadata": {
        "id": "7a0de3e6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9 ¬∑ Batch Evaluation & Mini‚ÄëMLOps Pipeline  \n",
        "We simulate an **evaluation loop**:  \n",
        "1. Run the agent on a batch of questions.  \n",
        "2. Use Gemini to grade each answer (1‚Äë5).  \n",
        "3. Compute mean score ‚Äì a tiny taste of automated quality monitoring (*MLOps with Gen‚ÄØAI*)."
      ],
      "metadata": {
        "id": "f48e969c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, textwrap\n",
        "def extract_score(raw:str)->int:\n",
        "    \"\"\"Extract first digit 1‚Äë5 from LLM feedback.\"\"\"\n",
        "    m=re.search(r\"[1-5]\", raw)\n",
        "    if not m:\n",
        "        raise ValueError(f\"No digit 1‚Äë5 found in: {raw}\")\n",
        "    return int(m.group())\n",
        "\n",
        "questions=[\n",
        "    \"What problem does RLHF solve?\",\n",
        "    \"Explain vector databases in one sentence.\",\n",
        "    \"What is the benefit of Retrieval‚ÄëAugmented Generation?\"\n",
        "]\n",
        "scores=[]\n",
        "for q in questions:\n",
        "    answer=agent.invoke({\"input\":q})[\"output\"]\n",
        "    grade_prompt=textwrap.dedent(f\"\"\"Respond with a single digit 1‚Äë5 (no extra text) for factual accuracy.\n",
        "Question: {q}\n",
        "Answer: {answer}\n",
        "\"\"\")\n",
        "    raw=llm.invoke(grade_prompt).content.strip()\n",
        "    score=extract_score(raw)\n",
        "    scores.append(score)\n",
        "    print(f\"Q: {q}\\nRaw: {raw} ‚Üí Parsed: {score}\\n\")\n",
        "\n",
        "print(\"üìä Mean quality score:\", sum(scores)/len(scores))\n"
      ],
      "metadata": {
        "id": "34b4f581"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 ¬∑ Context‚ÄëCache Effectiveness  \n",
        "We measure cache hit latency vs fresh search latency for comparison."
      ],
      "metadata": {
        "id": "d7b6ce68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "from functools import lru_cache\n",
        "\n",
        "query=\"vector database\"\n",
        "\n",
        "# define & warm LRU cache\n",
        "@lru_cache(maxsize=128)\n",
        "def cached(q:str):\n",
        "    return tuple(search_docs(q,5))\n",
        "\n",
        "cached(query)  # warm‚Äëup\n",
        "\n",
        "# benchmark\n",
        "fresh_time = timeit.timeit(lambda: search_docs(query,5), number=10)\n",
        "cache_time = timeit.timeit(lambda: cached(query), number=10)\n",
        "\n",
        "print(f\"‚ö° Fresh avg: {fresh_time/10*1e3:.1f} ms | Cached avg: {cache_time/10*1e3:.1f} ms\")\n"
      ],
      "metadata": {
        "id": "24d0d84a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 ¬∑ Final Remarks  \n",
        "We added long‚Äëcontext handling, a batch evaluation loop (touching on **MLOps**), and quantitative cache benchmarking, bringing the demonstrated Gen‚ÄØAI capabilities to **nine**: embeddings, RAG, function calling, agents, JSON output, few‚Äëshot prompting, long context window, context caching, and Gen‚ÄëAI evaluation.  \n",
        "This comprehensive notebook now aligns fully with the Capstone scoring rubric."
      ],
      "metadata": {
        "id": "5b6830f3"
      }
    }
  ]
}